{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"qliP-yurmxWC"},"outputs":[],"source":["!pip install -q Pillow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IF9qPZiCmvGr"},"outputs":[],"source":["import os\n","from PIL import Image\n","import numpy as np\n","import tensorflow as tf"]},{"cell_type":"markdown","metadata":{"id":"do63x1qZmqd_"},"source":["## Data Gathering"]},{"cell_type":"markdown","metadata":{"id":"ZvXvBS1VnATe"},"source":["Download dataset and extract to `train_data` and `val_data`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10275,"status":"ok","timestamp":1609008811028,"user":{"displayName":"Ly Nguyen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1nBUdqgyGPrTg8lXGC8cHEEjZ06cg4eNIe54J=s64","userId":"01585632328413993117"},"user_tz":-420},"id":"dJt89tXsfI0E","outputId":"35cf0658-42d8-4392-b84e-9d764f0ed337"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2020-12-26 18:53:26--  http://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.79.128, 108.177.96.128, 108.177.119.128, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.79.128|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 149574867 (143M) [application/zip]\n","Saving to: ‘horse-or-human.zip’\n","\n","horse-or-human.zip  100%[===================\u003e] 142.65M  68.7MB/s    in 2.1s    \n","\n","2020-12-26 18:53:29 (68.7 MB/s) - ‘horse-or-human.zip’ saved [149574867/149574867]\n","\n","--2020-12-26 18:53:29--  http://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 172.217.218.128, 74.125.128.128, 74.125.143.128, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|172.217.218.128|:80... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 11480187 (11M) [application/zip]\n","Saving to: ‘validation-horse-or-human.zip’\n","\n","validation-horse-or 100%[===================\u003e]  10.95M  36.4MB/s    in 0.3s    \n","\n","2020-12-26 18:53:30 (36.4 MB/s) - ‘validation-horse-or-human.zip’ saved [11480187/11480187]\n","\n"]}],"source":["!wget http://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip\n","!wget http://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rMZnLP39fbzU"},"outputs":[],"source":["TRAIN_DIR = \"./train_data\"\n","VAL_DIR = \"./val_data\"\n","!unzip -q horse-or-human.zip -d $TRAIN_DIR\n","!unzip -q validation-horse-or-human.zip -d $VAL_DIR"]},{"cell_type":"markdown","metadata":{"id":"PJ2wRkgfnP1n"},"source":["Read a sample image and check pixel range."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X7DYtpNXldJB"},"outputs":[],"source":["train_sample_fn = os.listdir(os.path.join(TRAIN_DIR, \"horses\"))[0]\n","train_sample_path = os.path.join(TRAIN_DIR, \"horses\", train_sample_fn)"]},{"cell_type":"markdown","metadata":{"id":"jD-D_FlhuQaz"},"source":["**Note:** The last channel is transparency."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3340,"status":"ok","timestamp":1609008814385,"user":{"displayName":"Ly Nguyen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj1nBUdqgyGPrTg8lXGC8cHEEjZ06cg4eNIe54J=s64","userId":"01585632328413993117"},"user_tz":-420},"id":"6s_waGxWi-zk","outputId":"2fbf0595-eebd-400e-a4af-3d6468b9a3ee"},"outputs":[{"name":"stdout","output_type":"stream","text":["Image shape: (300, 300, 4)\n","Pixel in range: 1 255\n","[[[100 108 123 255]\n","  [100 108 123 255]\n","  [ 99 108 123 255]\n","  ...\n","  [159 166 180 255]\n","  [163 171 185 255]\n","  [169 178 191 255]]]\n"]}],"source":["img = Image.open(train_sample_path)\n","img_data = np.asarray(img)\n","\n","print(\"Image shape:\", img_data.shape)\n","print(\"Pixel in range:\", np.min(img_data), np.max(img_data))\n","\n","print(img_data[:1])"]},{"cell_type":"markdown","metadata":{"id":"2_rqpGVVoQPw"},"source":["## Data Flow"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B--ZxrkMqZjL"},"outputs":[],"source":["img_size = (300, 300)\n","img_shape = (*img_size, 3)    # We didn't include transparency channel"]},{"cell_type":"markdown","metadata":{"id":"T-d1RouhobBt"},"source":["Create data flow from a directory, each image is scaled to the range 0-255."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eoHgnUAckbGV"},"outputs":[],"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","def gen_new_data(data_folder):\n","  data_gen = ImageDataGenerator(rescale=1./255)\n","  data = data_gen.flow_from_directory(\n","      data_folder,\n","      target_size=img_size,\n","      batch_size=64,\n","      class_mode=\"binary\",\n","  )\n","  return data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QoRQk6v1hYck"},"outputs":[],"source":["train_data_gen = gen_new_data(TRAIN_DIR)\n","val_data_gen = gen_new_data(VAL_DIR)"]},{"cell_type":"markdown","metadata":{"id":"79TfCpsMdptV"},"source":["## Load Pretrained Model"]},{"cell_type":"markdown","metadata":{"id":"ZAkkPfEkr5pO"},"source":["Load InceptionV3 pretrained on ImageNet and freeze all layers above `mixed7`. This layer will give us features extracted from InceptionV3. We're gonna replace the top of the model with our own classifier."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uXUSfTndd_QP"},"outputs":[],"source":["from tensorflow.keras.applications.inception_v3 import InceptionV3\n","pretrained_model = InceptionV3(include_top=False, weights=\"imagenet\", input_shape=(150, 150, 3))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qN9Wu2XZoXMb"},"outputs":[],"source":["# Freeze all layers\n","for layer in pretrained_model.layers:\n","  layer.trainable = False\n","\n","# We keep all layers up to `mixed7`\n","last_pretrained_layer = pretrained_model.get_layer(name=\"mixed7\")\n","print(\"Last layer shape:\", last_pretrained_layer.output_shape)"]},{"cell_type":"markdown","metadata":{"id":"mOCR_2sOo0jZ"},"source":["## Model Architecture \u0026 Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"40yHxP5Zoswt"},"outputs":[],"source":["from tensorflow.keras import Model\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout\n","from tensorflow.keras.layers.experimental.preprocessing import Rescaling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bhQHh_J7ixu_"},"outputs":[],"source":["train_model = Flatten()(last_pretrained_layer.output)\n","train_model = Dense(1024, activation=\"swish\")(train_model)\n","train_model = Dropout(0.1)(train_model)\n","train_model = Dense(1, activation=\"sigmoid\")(train_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pgJTRlZemSN5"},"outputs":[],"source":["model = Model(pretrained_model.input, train_model)\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0E8deF5imo5A"},"outputs":[],"source":["optimizer = tf.keras.optimizers.RMSprop(learning_rate=1e-4)\n","model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n","model.fit(train_data_gen, epochs=5, validation_data=val_data_gen)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"40ly_nU-yIwO"},"outputs":[],"source":["model.evaluate(val_data_gen)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XRlVkaNxImJQ"},"outputs":[],"source":["    model.save(\"mymodel.h5\")"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyMST/El6Qg4Hgkp1ZwEN76e","collapsed_sections":[],"name":"HorsePeople.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}